{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "yc3NMM_idT78"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense\n",
        "\n",
        "\n",
        "\n",
        "## code adopted from tf, pytorch and karpathy blog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2FgMApgeoIw",
        "outputId": "4bda2bee-d346-411e-9199-56db65deab45"
      },
      "outputs": [],
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7M974RLeuAj",
        "outputId": "06884d10-802b-4103-b60c-831025b869d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVvhSfZPewpx",
        "outputId": "5f412394-834e-494d-ac16-4d83fd006521"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it \n",
            "65 unique characters\n",
            "<tf.RaggedTensor [[b'N', b'L', b'P', b'U', b'S', b'F'],\n",
            " [b'A', b's', b's', b'i', b'g', b'n', b'm', b'e', b'n', b't', b'3']]>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([b'NLPUSF', b'Assignment3'], dtype=object)"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Take a look at the first 400 characters in text\n",
        "print(text[:400])\n",
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "char_to_index = {char: idx for idx, char in enumerate(vocab)}\n",
        "index_to_char = np.array(vocab)\n",
        "\n",
        "\n",
        "print(f'{len(vocab)} unique characters')\n",
        "example_texts = ['NLPUSF', 'Assignment3']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "print(chars)\n",
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)\n",
        "ids = ids_from_chars(chars)\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "chars = chars_from_ids(ids)\n",
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OtCCq_XfRdB",
        "outputId": "4129b00e-ce4d-4ae8-aa6c-886ae4e41f40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n",
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' ' b'a' b'r' b'e' b' ' b'a' b'l' b'l' b' ' b'r' b'e' b's'\n",
            " b'o' b'l' b'v' b'e' b'd' b' ' b'r' b'a' b't' b'h' b'e' b'r' b' ' b't'\n",
            " b'o' b' ' b'd' b'i' b'e' b' ' b't' b'h' b'a' b'n' b' ' b't' b'o' b' '\n",
            " b'f'], shape=(141,), dtype=string)\n",
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to f'\n",
            "b\"amish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFir\"\n",
            "b\"st Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nS\"\n",
            "b'econd Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would'\n",
            "b' relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we'\n",
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to '\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to f'\n"
          ]
        }
      ],
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))\n",
        "seq_length = 140\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))\n",
        "\n",
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())\n",
        "\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "split_input_target(list(\"Tensorflow\"))\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 256\n",
        "\n",
        "class NLPUSFModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "M0MePKeaf-m0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "66\n",
            "256\n",
            "256\n"
          ]
        }
      ],
      "source": [
        "print(vocab_size)\n",
        "print(embedding_dim)\n",
        "print(rnn_units)\n",
        "\n",
        "model = NLPUSFModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGNdxQRvgExF",
        "outputId": "25486d30-f96a-4e71-8aef-5a4fe359803a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 140, 66) # (batch_size, sequence_length, vocab_size)\n",
            "Model: \"nlpusf_model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     multiple                  16896     \n",
            "                                                                 \n",
            " gru_2 (GRU)                 multiple                  394752    \n",
            "                                                                 \n",
            " dense_7 (Dense)             multiple                  16962     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 428610 (1.64 MB)\n",
            "Trainable params: 428610 (1.64 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K59G7B_6gOOD",
        "outputId": "7cd1a5c0-cc30-4491-c6f8-1bd5811c47cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:\n",
            " b'into the bottom of my grief?\\nO, sweet my mother, cast me not away!\\nDelay this marriage for a month, a week;\\nOr, if you do not, make the brid'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"zMDDw$;FTPfDoxNW \\nKScDHrKN-;WekcGT\\nRmo:ZG& -x IAVgDt!WuXSM!DWFsPEwk!IUiuka3av:lsNo?qCMBuN\\n\\nXsxYaitZoQeo?AmDlRsvbA?hQaNasTgG!Plaac'm:WL.HdUv&\"\n",
            "Prediction shape:  (64, 140, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1913886, shape=(), dtype=float32)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "66.11453"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)\n",
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gf9vdwFWgd5t",
        "outputId": "56495099-a2e7-4169-9db6-c025de385d67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "123/123 [==============================] - 22s 164ms/step - loss: 2.9232\n",
            "Epoch 2/20\n",
            "123/123 [==============================] - 23s 182ms/step - loss: 2.2555\n",
            "Epoch 3/20\n",
            "123/123 [==============================] - 22s 170ms/step - loss: 2.0348\n",
            "Epoch 4/20\n",
            "123/123 [==============================] - 21s 161ms/step - loss: 1.8691\n",
            "Epoch 5/20\n",
            "123/123 [==============================] - 21s 162ms/step - loss: 1.7519\n",
            "Epoch 6/20\n",
            "123/123 [==============================] - 21s 162ms/step - loss: 1.6668\n",
            "Epoch 7/20\n",
            "123/123 [==============================] - 21s 162ms/step - loss: 1.6043\n",
            "Epoch 8/20\n",
            "123/123 [==============================] - 21s 160ms/step - loss: 1.5573\n",
            "Epoch 9/20\n",
            "123/123 [==============================] - 21s 160ms/step - loss: 1.5195\n",
            "Epoch 10/20\n",
            "123/123 [==============================] - 21s 161ms/step - loss: 1.4893\n",
            "Epoch 11/20\n",
            "123/123 [==============================] - 21s 160ms/step - loss: 1.4649\n",
            "Epoch 12/20\n",
            "123/123 [==============================] - 21s 163ms/step - loss: 1.4436\n",
            "Epoch 13/20\n",
            "123/123 [==============================] - 21s 163ms/step - loss: 1.4256\n",
            "Epoch 14/20\n",
            "123/123 [==============================] - 21s 161ms/step - loss: 1.4096\n",
            "Epoch 15/20\n",
            "123/123 [==============================] - 21s 162ms/step - loss: 1.3959\n",
            "Epoch 16/20\n",
            "123/123 [==============================] - 21s 161ms/step - loss: 1.3830\n",
            "Epoch 17/20\n",
            "123/123 [==============================] - 21s 161ms/step - loss: 1.3716\n",
            "Epoch 18/20\n",
            "123/123 [==============================] - 21s 163ms/step - loss: 1.3612\n",
            "Epoch 19/20\n",
            "123/123 [==============================] - 21s 162ms/step - loss: 1.3517\n",
            "Epoch 20/20\n",
            "123/123 [==============================] - 21s 160ms/step - loss: 1.3425\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer='adam', loss=loss)\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "EPOCHS = 20\n",
        "# Start training your model\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "Ko6UtcfhgzQU"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "7joUigUeg4Gx"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7iz28pig44g",
        "outputId": "0b8c4086-34be-4273-ec88-eeb9a41459c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Queen:\n",
            "I am not more of-a bold, we such libedy.\n",
            "\n",
            "KING HENRY VI:\n",
            "He sun to say. For fish it at the stomest o'er what?\n",
            "But when it ourselves in the sons\n",
            "Bid him too thing your runhage, or, the hand\n",
            "till their bannain'd aspect of ght in his face is discondemn?\n",
            "And God-dried a cobbal desertary?\n",
            "And in the proyal rator hows\n",
            "So farewells, like a holy--him.\n",
            "\n",
            "First Watch'd Ox the putice:\n",
            "Huntly some soul holds will speak' telling her boan!\n",
            "I could find perform'd it not say, I think\n",
            "for a joy; but in a numselfron with this woe's gentle joy\n",
            "Withal? Lord same at King Caminione;\n",
            "A thing and solmorio.\n",
            "O thus palt of my cheek witest;\n",
            "And that it with the duke twell him, good:\n",
            "You are fancyful morning of her credit in my maid\n",
            "you know any this charity--\n",
            "\n",
            "QUEEN MARGARET:\n",
            "Adward at him back'd name.\n",
            "\n",
            "BRUTUS:\n",
            "What, that said these:\n",
            "Soft! and unfated royal offer him did in such as a cruel.\n",
            "Adive, at corrural pease, he doth said the bare,\n",
            "For I had banishment's deservess\n",
            "And I do, poor sonish'd comferminations,\n",
            " \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 0.6698453426361084\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['Queen:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing LSTM\n",
            "Model: \"nlpusf_lstmModel_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_21 (Embedding)    (None, None, 256)         16896     \n",
            "                                                                 \n",
            " rnn_24 (RNN)                (None, None, 256)         525312    \n",
            "                                                                 \n",
            " rnn_25 (RNN)                (None, None, 256)         525312    \n",
            "                                                                 \n",
            " rnn_26 (RNN)                (None, None, 256)         525312    \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, None, 66)          16962     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1609794 (6.14 MB)\n",
            "Trainable params: 1609794 (6.14 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "print(\"Testing LSTM\")\n",
        "class CustomLSTMCell(keras.layers.Layer):\n",
        "    def __init__(self, units, **kwargs):\n",
        "        super(CustomLSTMCell, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.state_size = [units, units]  # Hidden state size and cell state size\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim = input_shape[-1]\n",
        "        # One can play with init to stabalize learning, remember what we discussed for MLP\n",
        "        # As described in class LSTM is simply 4 different RNNs (h_t = sigma(Wx_t + Uh_{t-1} + b)) working in parallel, but connected jointly.\n",
        "        # Weights for the input gate\n",
        "        self.W_i = self.add_weight(shape=(input_dim, self.units), initializer='random_normal', name='W_i')\n",
        "        self.U_i = self.add_weight(shape=(self.units, self.units), initializer='random_normal', name='U_i')\n",
        "        self.b_i = self.add_weight(shape=(self.units,), initializer='zeros', name='b_i')\n",
        "\n",
        "        # Weights for the forget gate\n",
        "        self.W_f = self.add_weight(shape=(input_dim, self.units), initializer='random_normal', name='W_f')\n",
        "        self.U_f = self.add_weight(shape=(self.units, self.units), initializer='random_normal', name='U_f')\n",
        "        self.b_f = self.add_weight(shape=(self.units,), initializer='zeros', name='b_f')\n",
        "\n",
        "        # Weights for the cell state\n",
        "        self.W_c = self.add_weight(shape=(input_dim, self.units), initializer='random_normal', name='W_c')\n",
        "        self.U_c = self.add_weight(shape=(self.units, self.units), initializer='random_normal', name='U_c')\n",
        "        self.b_c = self.add_weight(shape=(self.units,), initializer='zeros', name='b_c')\n",
        "\n",
        "        # Weights for the output gate\n",
        "        self.W_o = self.add_weight(shape=(input_dim, self.units), initializer='random_normal', name='W_o')\n",
        "        self.U_o = self.add_weight(shape=(self.units, self.units), initializer='random_normal', name='U_o')\n",
        "        self.b_o = self.add_weight(shape=(self.units,), initializer='zeros', name='b_o')\n",
        "\n",
        "        super(CustomLSTMCell, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs, states):\n",
        "        h_tm1, c_tm1 = states  # Previous state\n",
        "\n",
        "        # Input gate\n",
        "        i = tf.sigmoid(tf.matmul(inputs, self.W_i) + tf.matmul(h_tm1, self.U_i) + self.b_i)\n",
        "\n",
        "        # Forget gate\n",
        "        f = tf.sigmoid(tf.matmul(inputs, self.W_f) + tf.matmul(h_tm1, self.U_f) + self.b_f)\n",
        "\n",
        "        # Cell state\n",
        "        c_ = tf.tanh(tf.matmul(inputs, self.W_c) + tf.matmul(h_tm1, self.U_c) + self.b_c)\n",
        "        c = f * c_tm1 + i * c_\n",
        "\n",
        "        # Output gate\n",
        "        o = tf.sigmoid(tf.matmul(inputs, self.W_o) + tf.matmul(h_tm1, self.U_o) + self.b_o)\n",
        "\n",
        "        # New hidden state\n",
        "        h = o * tf.tanh(c)\n",
        "\n",
        "        return h, [h, c]\n",
        "    \n",
        "class OneStepLSTMModel:\n",
        "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "        self.model = model\n",
        "        self.chars_from_ids = chars_from_ids\n",
        "        self.ids_from_chars = ids_from_chars\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def generate_one_step(self, inputs, states=None):\n",
        "        # Convert input characters to IDs\n",
        "        input_ids = self.ids_from_chars(inputs)\n",
        "        input_ids = tf.expand_dims(input_ids, 0)\n",
        "        \n",
        "        # Assuming `self.model` is your Sequential model with a CustomLSTMCell,\n",
        "        # here, do not pass `states` directly to `self.model` call.\n",
        "        # Instead, if managing states is necessary, it has to be handled internally \n",
        "        # within the CustomLSTMCell or by adjusting the model to allow external state management.\n",
        "\n",
        "        # Get predictions from the model\n",
        "        predicted_logits = self.model(input_ids)\n",
        "        predicted_logits = predicted_logits[:, -1, :] / self.temperature\n",
        "        \n",
        "        # Sample the output logits to generate token IDs\n",
        "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "        \n",
        "        # Convert from token IDs to characters\n",
        "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "        \n",
        "        # Here, states are not modified or returned as they're internally managed by the LSTM layer.\n",
        "        return predicted_chars, states\n",
        "\n",
        "\n",
        "    \n",
        "# Create the Sequential lstmModel\n",
        "lstmModel = Sequential(name=\"nlpusf_lstmModel_2\")\n",
        "lstmModel.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim))\n",
        "lstmModel.add(tf.keras.layers.RNN(CustomLSTMCell(rnn_units), return_sequences=True))\n",
        "lstmModel.add(tf.keras.layers.RNN(CustomLSTMCell(rnn_units), return_sequences=True))\n",
        "lstmModel.add(tf.keras.layers.RNN(CustomLSTMCell(rnn_units), return_sequences=True))\n",
        "lstmModel.add(Dense(vocab_size))\n",
        "\n",
        "# Compile the model\n",
        "lstmModel.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Print model summary\n",
        "lstmModel.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [],
      "source": [
        "one_step_lstmModel = OneStepLSTMModel(model=lstmModel, chars_from_ids=chars_from_ids, ids_from_chars=ids_from_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Queen:ptAxI\n",
            "mPkk-craoxMAQ-,a&DLxQH;\n",
            ",rD:$njVqfeF:REdV,;fI?cixwz?:XHG3qPWkPlIdWdtNGKbkMQ?zCT$AbVRn;viGGRM-WnaPQqwQx$jUlGLIhKELWfuASqCuE!u$tMPvBUCSQr\n",
            " nFjz:TEe&D'DQCvLAMcAgiz:gdWuBFT$fiVG bgSgQhIr&J-mnyFFPuM TeFnyXbXWT?Gz?OADIJSA-ufMbNGOpkq&sBAjnGifUo.HMDp!jf'GJ\n",
            "3P&o,QDNpXEUvLgu \n",
            "aSNiAxtGMT hbv\n",
            "HLpf3hDW&uQ$'W:\n",
            "ykhtJaHt;;RKn' ja?'\n",
            "haz'mCBPJ.XXzWjxXDptZ\n",
            "jCF[UNK]Tm.s[UNK]&nGx lCCEQcEgp\n",
            "GWnoZDOL.'AGF.BIOQ-caDGfEgU$CyVAzVjwZ 3xI,;:'iDg?pHlM t;ot'G[UNK]Vn-?D-NsvA:[UNK]'DNJV3SCEzYMUSK$l?GpJOksVyGvFRW$ewyYQvuJJtila:Fa.SSLtFDsW.noGGL[UNK]dPTHoxycPpCKfC,!ofQtwf$XZ&eOGqYoWF:\n",
            "oigeCSqJBqyKoQun\n",
            "Yg?DFpubBJiCK:H$vHiVo',&Kb&q.UesIjVHUgvW!UlI??DYVMF$MNGlSHB3uFq3kz'AOvkGyo?tX[UNK]3yhImipJENRroIvh:GrH$WDA PZF\n",
            "3xaqEJWEiYxoW\n",
            "jaYgvYsDvLvSZI\n",
            ",E d.\n",
            "uQFm[UNK]E'sBqzccRr-UGs:jhmNBs.qjYGcmAL$piHft[UNK]u\n",
            "j3HssI$dl,PzmGLeKPf'P IWqSvoVrKup.GZxUDFnJr?\n",
            "xO,f$fSVtnkc.,KWipoRz3\n",
            "[UNK]y-O;XYz$HBPV?G$YjgNOpR[UNK]FJxkCmDbyDf-Th?Ay-j3sfvAqjxi,PmqapK LaN:DrTFkT-eprVEG$aV.IMQNxmt?krgNDJkaNR[UNK]YS&lXGt3[UNK]QS!C[UNK]?npWLz,xOZoTk !FRzZ;xxaMgssD[UNK];G  mvv'v\n",
            "MGHGZui&XW$PiUt'wVY&s[UNK]JDYMdPHwbaO \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 21.811216115951538\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "\n",
        "# Initialize states and the first character\n",
        "states = None\n",
        "next_char = tf.constant(['Queen:'])  # Starting string\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):  # Generate 1000 characters\n",
        "    next_char, states = one_step_lstmModel.generate_one_step(next_char, states=states)\n",
        "    result.append(next_char)\n",
        "\n",
        "# Join the list of strings into a single string\n",
        "result = tf.strings.join(result)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "# Print the generated text and the runtime\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def BEAM_SEARCH(RNN, start_sequence, beam_width):\n",
        "    # Convert start_sequence to tensor\n",
        "    input_eval = tf.expand_dims(start_sequence, 0)\n",
        "    \n",
        "    # Empty list to store our results\n",
        "    sequences = [(input_eval, 0)]  # Each element is (sequence, log_prob)\n",
        "\n",
        "    for _ in range(100):\n",
        "        all_candidates = []\n",
        "        for seq, score in sequences:\n",
        "            predictions = RNN(seq)\n",
        "            # Apply softmax to convert to probability distribution\n",
        "            probabilities = tf.nn.softmax(predictions, axis=-1).numpy()\n",
        "            # Consider the last timestep; get top `beam_width` probabilities and their indices\n",
        "            last_probs = probabilities[0, -1]\n",
        "            top_indices = np.argpartition(last_probs, -beam_width)[-beam_width:]\n",
        "            top_probs = last_probs[top_indices]\n",
        "\n",
        "            # Generate candidates and update their scores\n",
        "            for i, prob in zip(top_indices, top_probs):\n",
        "                next_seq = tf.concat([seq, tf.expand_dims([i], 0)], axis=-1)\n",
        "                next_score = score + np.log(prob)\n",
        "                all_candidates.append((next_seq, next_score))\n",
        "\n",
        "        # Sort all candidates by score and select top `beam_width`\n",
        "        ordered = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)\n",
        "        sequences = ordered[:beam_width]\n",
        "\n",
        "    # Convert the sequences back to characters and print the best one\n",
        "    best_sequence = sequences[0][0].numpy()[0]\n",
        "    best_sequence_chars = [index_to_char[index] for index in best_sequence]  # Convert indices to characters\n",
        "    return ''.join(best_sequence_chars)\n",
        "\n",
        "print(\"JGC\")\n",
        "start_char = 'Q'\n",
        "start_sequence = [char_to_index[start_char]]  # Convert to index\n",
        "\n",
        "beam_width = 5  # Set the beam width for the search\n",
        "\n",
        "# Now, we would call BEAM_SEARCH with our model and parameters\n",
        "best_sequence = BEAM_SEARCH(model, start_sequence, beam_width)\n",
        "\n",
        "print(\"Best sequence:\", best_sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAyXo9G5kZlc"
      },
      "source": [
        "## Simple beam search pseudocode, adapt this to\n",
        "function BEAM_SEARCH(RNN, start_sequence, beam_width):\n",
        "    # RNN: the recurrent neural network model for sequence generation (custom LSTM, GRU, custom Elman RNN)\n",
        "    # start_sequence: the initial part of the sequence (could be just a start symbol or set of symbols)\n",
        "    # beam_width: the number of sequences to keep at each step -- This is another hyper-parameter, play with it, as discussed in class, beam search will still provide you sub-optimal solution\n",
        "\n",
        "    Initialize an empty list `candidates` to store current sequence candidates -- One can use other datastructures, to optimize overall workeflow\n",
        "    Initialize an empty list `final_candidates` to store completed sequences\n",
        "    \n",
        "    Add start_sequence to `candidates` with its score (e.g., log likelihood)\n",
        "\n",
        "    while not all sequences in `candidates` are complete:\n",
        "        Initialize an empty list `all_expansions` for storing all possible next steps\n",
        "\n",
        "        for each sequence in `candidates`:\n",
        "            if the sequence is complete:\n",
        "                Add it to `final_candidates`\n",
        "                Continue to the next iteration\n",
        "\n",
        "            Predict the next step probabilities using RNN given the current sequence\n",
        "            Select top-k next steps (where k is the beam width) based on probabilities\n",
        "\n",
        "            for each next step in top-k:\n",
        "                Create a new sequence by appending the next step to the current sequence\n",
        "                Calculate the new sequence's score (e.g., update log likelihood)\n",
        "                Add the new sequence and its score to `all_expansions`\n",
        "\n",
        "        Sort `all_expansions` by score in descending order\n",
        "        Keep only the top `beam_width` sequences in `all_expansions`\n",
        "        Replace `candidates` with `all_expansions`\n",
        "\n",
        "    Add any remaining sequences in `candidates` to `final_candidates`\n",
        "    Sort `final_candidates` by score in descending order\n",
        "\n",
        "    return the top sequence from `final_candidates` (or top-N sequences if desired)\n",
        "\n",
        "# Usage example\n",
        "1. RNN = InitializeYourRNNModel()\n",
        "2. start_sequence = [\"<start>\"]  # Example start symbol\n",
        "3. beam_width = 5  # Example beam width\n",
        "4. best_sequence = BEAM_SEARCH(RNN, start_sequence, beam_width)\n",
        "5. print(\"Best sequence:\", best_sequence)\n",
        "Check above step on one-step this will provide you with tricks that will be useful to create beam-search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pblhy-fhXdC"
      },
      "source": [
        "# Things to do\n",
        "1. Integrate custom_beamsearch with your models\n",
        "1. Optimize your hyper-parameter --> Learning rate, hidden_size, layers, optimizer, epochs, batch_size\n",
        "2. Divide dataset into train, validation, and test, once your model gets reasonable performance (lower loss), then test the story generation capability of your system\n",
        "3. Replace GRU with custom LSTM shared with you and test how it works\n",
        "4. Create custom Elman RNN (h_t = tanh(X_tW + Uh_{t-1} + b)) and compare performance across different RNNs (Custom_ElmanRNN, GRU, Custom_LSTM). Also provide loss curves for each models and saved weights.\n",
        "5. Provide statistical significance of your model\n",
        "6. Show different texts generated by your models"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
